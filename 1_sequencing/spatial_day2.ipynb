{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489900f2-a867-4bcd-9264-deff14317fe1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DAY 2: Visium Spatial Transcriptomics Data Analysis - Mouse Intestine\n",
    "\n",
    "This notebook will guide you through Day 2 content.\n",
    "We will cover:\n",
    "\n",
    "1. Cell type deconvolution\n",
    "2. Spatial Neighbours\n",
    "3. Working with Multiple Tissue Sections\n",
    "4. High resolution Visium HD Data Analysis\n",
    "\n",
    "## How to use this notebook\n",
    "\n",
    "This notebook is intended to be used as a reference for you own analysis.\n",
    "All code chunks have an explanation detailing the analysis steps and their purpose, as well as key parameters.\n",
    "Play around with these and see what they do, so that you are better equiped to adapt the workflow to your own data.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "We will be using a Visium dataset from Parigi et al, 2022.\n",
    "This dataset was generated using V1 3' polyA Visium chemistry and consists of four mouse intestine samples taken from healthy mice and mice subjected to a DSS colitis model, where the intestine is damaged.\n",
    "Today, we will be using healthy mouse intestine section as an example.\n",
    "Overall, the dataset quality is good but there are some quality issues, which will hopefully show you what to look out for in your own data. \n",
    "\n",
    "https://www.nature.com/articles/s41467-022-28497-0\n",
    "\n",
    "For HD data analysis, we will be using a public demo dataset from 10x Genomics, also from mouse intestine. The tissues are very similar, so this should give you a good sense of the differences between older and new technologies.\n",
    "\n",
    "https://www.10xgenomics.com/datasets/visium-hd-cytassist-gene-expression-libraries-of-mouse-intestine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1871f8b-3826-4883-96b9-cd45ec3e0dbe",
   "metadata": {},
   "source": [
    "## 1. Cell Type Deconvolution\n",
    "\n",
    "Many ST platforms capture gene expression data at resolutions that encompass multiple cells within each spatial spot.\n",
    "In this case, each Visium spot is approximately 55 micrometres in diameter and can cover multiple cell types, particularly in dense or heterogeneous tissues.\n",
    "This mixing of cell types within a single spot poses a significant challenge for understanding the cellular composition and the spatial organisation of individual cell types.\n",
    "\n",
    "Cell type deconvolution is the process of estimating the proportions and types of different cells present within a mixed cell population, based on gene expression data.\n",
    "In the context of spatial transcriptomics, deconvolution methods aim to unravel the cellular composition of each spot by inferring which cell types are present and in what proportions.\n",
    "This process is crucial for interpreting the spatial organisation of cell types within tissues.\n",
    "\n",
    "### Approaches to Cell Type Deconvolution\n",
    "\n",
    "Several computational approaches have been developed to perform cell type deconvolution in spatial transcriptomics data. These methods generally fall into two categories:\n",
    "\n",
    "**Reference-Based Methods:**\n",
    "These methods use reference expression profiles from single-cell RNA-seq datasets to infer the cell type composition of spatial spots. They typically involve algorithms that match the expression profiles of spots to those of known cell types.\n",
    "\n",
    "**De Novo Methods:** These methods do not rely on external reference datasets and instead use statistical models to infer transcriptional groups from the data directly.\n",
    "\n",
    "### Considerations and Challenges\n",
    "\n",
    "**Resolution and Spot Size:**\n",
    "The resolution of the spatial platform affects the accuracy of deconvolution.\n",
    "Smaller spot sizes generally yield more precise cell type compositions but may also capture fewer transcripts, leading to increased noise.\n",
    "\n",
    "**Reference Data Quality:**\n",
    "The choice and quality of the reference single-cell dataset significantly influence deconvolution accuracy.\n",
    "The reference must be well-annotated and relevant to the tissue being studied.\n",
    "The majority of available methods are reference-based and these tend to perform better than reference-free approaches, but this entirely hinges on using an appropriate reference.\n",
    "\n",
    "**Reference Data Resolution:**\n",
    "Many deconvolution algorithms struggle to accurately distinguish closely related cell types;\n",
    "e.g. T-Cell vs B-Cell is straightforward, but CD4+ T-Cell vs CD8+ T-Cell (or even finer subsets than that) predictions should be interpreted more cautiously.\n",
    "\n",
    "Here, we will use a reference-based method [cell2location](https://github.com/BayraktarLab/cell2location).\n",
    "It is currently one of the best performing methods in Python, although many others are available.\n",
    "\n",
    "You can read more about the cell2location method here:\n",
    "\n",
    "https://www.nature.com/articles/s41587-021-01139-4\n",
    "\n",
    "As a reference, we will use a mouse intestine scRNA-Seq dataset that we prepared earlier.\n",
    "We can read in this data from a previously prepared [AnnData](https://anndata.readthedocs.io/en/stable/) object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b144a-2422-4855-9a28-0a78b3118fa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Set up your environment and load reference data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8657b5-3f18-448c-9159-0c50cebdb37b",
   "metadata": {},
   "source": [
    "First we need to set up the environment and load the packages we will use for this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f62495-dc0d-45fe-af64-711030f3e57e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os package for working with system path\n",
    "import os\n",
    "# import numpy for scientific computing \n",
    "import numpy as np\n",
    "# import pandas for dataframe manipulation\n",
    "import pandas as pd\n",
    "# import matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "# import Scanpy and AnnData for single-cell RNAseq\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "# import squidpy for spatial transcriptomics\n",
    "import squidpy as sq\n",
    "# import cell2location for cell type deconvolution\n",
    "import cell2location\n",
    "from cell2location.models import RegressionModel\n",
    "import scvi\n",
    "from scipy.stats import zscore\n",
    "from scipy.sparse import csr_matrix\n",
    "import bin2cell as b2c\n",
    "from spatialdata_io import visium_hd\n",
    "import spatialdata_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d7cc6-e101-4440-8ce1-77c02741865f",
   "metadata": {},
   "source": [
    "In the first part of today, we will be (optionally) using GPUs on the cluster to run parts of the code that would otherwise be very slow to run.\n",
    "To do this from jupyter, we will use a custom package that submits chunks of code to slurm queue and loads the outputs back to the jupyter environment.\n",
    "This is loaded by importing the package below. \n",
    "\n",
    "If you are using WIMM CCB cluster, this is also available there.\n",
    "If you are using a different cluster set up, you would need to set this up outside of jupyter, or run the notebook on a node with GPU access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0ac47-0fb5-4012-80c9-aacf51cd892f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipy_slurm_exec\n",
    "%load_ext ipy_slurm_exec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5ead8-a1f1-477d-ae8b-58fad7237600",
   "metadata": {},
   "source": [
    "Set location to load data from and store analysis output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91d1ca-b69e-4bd5-84ad-da936e89ab9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_FOLDERNAME = '/nvme/project/shared/python/5_python_spatial_omics/data'\n",
    "PRECOMPUTED_FOLDERNAME = '/nvme/project/shared/python/5_python_spatial_omics/data/precomputed/day1'\n",
    "OUTPUT_FOLDERNAME = '/PATH/TO/YOUR/DIRECTORY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcab5df-d861-49f2-aeee-27027a3933a4",
   "metadata": {},
   "source": [
    "To start with, we will load the anndata object we have worked on yesterday. A pre-computed one for the tutorial is available here if you don't want to use your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ffc4a-8672-485f-8b91-552535d53c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\n",
    "    os.path.join(PRECOMPUTED_FOLDERNAME, 'day1.h5ad')\n",
    ")\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55b1b5-ee9d-4700-a9dd-14def110b7df",
   "metadata": {},
   "source": [
    "Quick visualisation of the spatial data that we were using yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e469b-589a-430e-b4cc-6a17d63b286b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.pl.spatial_scatter(\n",
    "    adata,\n",
    "    color=\"clusters\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c8685-e4da-4db8-9373-66289c5c5ff0",
   "metadata": {},
   "source": [
    "Cell type deconvolution requires a single cell RNA-Seq reference dataset.\n",
    "Ideally, you'd have a matched reference to your experiment, although this is not always possible.\n",
    "If you don't have appropriate single cell data yourself, a good place to start looking for one is [CZI cellxgene data portal](https://cellxgene.cziscience.com/datasets), which has a large assortment of single-cell datasets from all major organs that you can download as [AnnData](https://anndata.readthedocs.io/en/stable/) objects.\n",
    "\n",
    "Here, we pre-prepared a mouse intestine single-cell reference dataset.\n",
    "Let's load it and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2451866-5b0b-4e79-a224-84bcf26bb2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref = sc.read_h5ad(\n",
    "    os.path.join(PRECOMPUTED_FOLDERNAME,'ref.h5ad')\n",
    ")\n",
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df7c0d-d6ea-40ea-bed6-0eb98a465647",
   "metadata": {},
   "source": [
    "Let’s visualise the dataset - here, we can see that the dataset has all major epithelial, mesenchymal and immune cells present in the tissue.\n",
    "\n",
    "> **Exercise:**\n",
    "What would happen if your reference was incomplete/missing key cell types?\n",
    "As an additional exercise, try removing cell populations from this reference to see how incomplete references impact deconvolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fa7c7-3a49-4258-b23a-023ca80c2831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    ref,\n",
    "    color=[\"CellType\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c69f9c3-3371-46fc-a893-f0c04598b3db",
   "metadata": {},
   "source": [
    "### 1.2 Filter data\n",
    "\n",
    "First we want to filter out genes from the reference dataset that are likely to be uninformative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd81d6d-18c4-459c-98e8-58f685f67acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected = cell2location.utils.filtering.filter_genes(\n",
    "    ref,\n",
    "    cell_count_cutoff=5,\n",
    "    cell_percentage_cutoff2=0.03,\n",
    "    nonz_mean_cutoff=1.12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449c82f-c5d9-4467-b830-9b5533aedd47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref = ref[:, selected].copy()\n",
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc5fe6-8c44-41b2-abfc-7eca1296c95b",
   "metadata": {},
   "source": [
    "Lets also remove mitochondrial and ribosomal genes from the main matrix as they're likely to be uninformative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b994b-19c8-4da0-8a53-bc58a7eb023a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mt = ref.var_names.str.startswith(\"mt-\")\n",
    "rb = ref.var_names.str.startswith(\"Rp\")\n",
    "\n",
    "ref.obsm[\"mt\"] = ref[:, mt].X.toarray()\n",
    "ref.obsm[\"rb\"] = ref[:, rb].X.toarray()\n",
    "\n",
    "ref = ref[:, ~(mt | rb)].copy()\n",
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d0812-e399-4084-9d1b-aa4559411318",
   "metadata": {},
   "source": [
    "### 1.3 Estimation of reference cell type signatures\n",
    "\n",
    "The signatures are estimated from scRNA-seq data, accounting for batch effect, using a Negative binomial regression model.\n",
    "\n",
    "First, we prepare anndata object for the regression model.\n",
    "Then, we train the model to estimate the reference cell type signatures.\n",
    "\n",
    "Note that to achieve convergence on your data (i.e., to get stabilization of the loss) you may need to increase `max_epochs=250` (see below).\n",
    "\n",
    "Note also that here we are using `batch_size=2500` which is much larger than [scvi-tools](https://scvi-tools.org/) default and performs training on all cells in the data (`train_size=1`) - both parameters are defaults.\n",
    "\n",
    "It is much faster to use GPU for training so we will use \"slurm magic\" to submit this cell to the GPU slurm queue for processing\n",
    "\n",
    "**NOTE:**\n",
    "this step can take a while (approx. 10 min), especially if running on CPU (much longer).\n",
    "Therefore, we have precomputed a model file for you here that you can load instead.\n",
    "To do so, feel free to skip to section `1.4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a129b41-89a9-48e0-b5d0-5b67dfdffc78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%slurm_exec -i ref, -o mod --time=00:20:00 --partition=gpu --gpus=1 --cpus=2 --mem=20G\n",
    "cell2location.models.RegressionModel.setup_anndata(\n",
    "    adata=ref,\n",
    "    batch_key='Sample',   # 10X reaction / sample / batch\n",
    "    labels_key='CellType' # cell type, covariate used for constructing signatures\n",
    ")\n",
    "mod = cell2location.models.RegressionModel(ref)\n",
    "mod.train(max_epochs=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec03d31-cba3-4e75-bea2-e9e9d200ef3c",
   "metadata": {},
   "source": [
    "Check for convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad053b-ca9f-4836-a228-94eae858c092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mod.plot_history(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff2eea-1cfb-4aff-a68c-064f24044e92",
   "metadata": {},
   "source": [
    "Next, we extract relevant model statistics and store them in [AnnData](https://anndata.readthedocs.io/en/stable/) reference object.\n",
    "The function samples the trained [cell2location](https://github.com/BayraktarLab/cell2location) model’s posterior and stores summary statistics (means/uncertainty) in the [AnnData](https://anndata.readthedocs.io/en/stable/).\n",
    "This makes the results reusable for plotting and downstream analysis without the model.\n",
    "\n",
    "As before, this step may take a while to run, so we have prepared a precomputed object.\n",
    "To do so, **skip to step** `1.4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58742b-d0d7-4063-a858-084e76a54eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%slurm_exec -i ref,mod -o ref,mod --time=00:20:00 --partition=gpu --gpus=1 --cpus=2 --mem=20G\n",
    "ref = mod.export_posterior(\n",
    "    ref,\n",
    "    sample_kwargs={\n",
    "        'num_samples': 1000,\n",
    "        'batch_size': 2500\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106390f7-515e-4023-9303-4dccfd0d0fef",
   "metadata": {},
   "source": [
    "### 1.4 Loading and saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4daaf4-69a6-400d-9e01-626acbf0a502",
   "metadata": {},
   "source": [
    "We can save the trained model to re-use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ae4f3-0e8d-48df-859b-8f120868c139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving your trained model\n",
    "mod.save(OUTPUT_FOLDERNAME, overwrite=True)\n",
    "\n",
    "adata_file = f\"{OUTPUT_FOLDERNAME}/ref.h5ad\"\n",
    "ref.write(adata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369c8e1-8fce-448f-9e2e-e41abc4a7764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reloading your trained model\n",
    "ref = sc.read_h5ad(os.path.join(OUTPUT_FOLDERNAME, 'ref.h5ad'))\n",
    "mod = RegressionModel.load(\n",
    "    OUTPUT_FOLDERNAME,\n",
    "    ref\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96dfbe6-7626-4601-b98d-e1e56247c8a7",
   "metadata": {},
   "source": [
    "The code below loads a model we pre-trained earlier, so if you trained your own, **skip this step**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e48a8-5b8e-4dde-b823-3c97379dc2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading our precomputed model\n",
    "ref = sc.read_h5ad(os.path.join(f\"{PRECOMPUTED_FOLDERNAME}/cell2location/\", 'ref.h5ad')) # TODO: file not found\n",
    "mod = RegressionModel.load(\n",
    "    f\"{PRECOMPUTED_FOLDERNAME}/cell2location/regression\",\n",
    "    ref\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1f9e7-32d1-4d04-ab63-2e43a7712200",
   "metadata": {},
   "source": [
    "### 1.5 Export estimated expression in each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1aa7d5-e188-4750-8fc6-62abb38d5f64",
   "metadata": {},
   "source": [
    "Next, we want to use our regression model to estimate average expression of each gene across all cell type labels in the reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dca633-6423-4b5b-8fda-1ca377b10c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'means_per_cluster_mu_fg' in ref.varm.keys():\n",
    "    inf_aver = ref.varm['means_per_cluster_mu_fg'][[f'means_per_cluster_mu_fg_{i}'\n",
    "                                                    for i in ref.uns['mod']['factor_names']]].copy()\n",
    "else:\n",
    "    inf_aver = ref.var[[f'means_per_cluster_mu_fg_{i}'\n",
    "                        for i in ref.uns['mod']['factor_names']]].copy()\n",
    "inf_aver.columns = ref.uns['mod']['factor_names']\n",
    "inf_aver.iloc[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749040b5-4faf-46a1-bc33-dd8e2b9426a3",
   "metadata": {},
   "source": [
    "To deconvolute cell types, we need to ensure that both the reference single-cell dataset and the spatial transcriptomics dataset use the same genes.\n",
    "If different references or pipelines were used to process the raw data, these might not be 100% the same between datasets.\n",
    "We also filtered out uninformative genes in the single-cell dataset.\n",
    "Finally, we find shared genes and subset both the [AnnData](https://anndata.readthedocs.io/en/stable/) object and the reference signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2065ce9-3be4-4e28-9438-dbb388a7f806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intersect = np.intersect1d(adata.var_names, inf_aver.index)\n",
    "adata = adata[:, intersect].copy()\n",
    "inf_aver = inf_aver.loc[intersect, :].copy()\n",
    "inf_aver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe5fce9-8197-46aa-8785-3e16f798e96f",
   "metadata": {},
   "source": [
    "[cell2location](https://github.com/BayraktarLab/cell2location) works on counts, so we need to move the original raw counts matrix back to `adata.X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d2698-7f3f-4011-9b60-235d76bb861d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.obs['sample'] = 'slide1'\n",
    "adata.X = adata.layers['counts'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90894adc-f4db-40b6-82fe-15eafe625313",
   "metadata": {},
   "source": [
    "### 1.6 Estimate cell abundance\n",
    "\n",
    "Next, we configure the [AnnData](https://anndata.readthedocs.io/en/stable/) object and initialise a [cell2location](https://github.com/BayraktarLab/cell2location) model with sample-level batching and prior information on cell states.\n",
    "\n",
    "We then train the model on all spatial spots to estimate cell-type abundances everywhere in the tissue.\n",
    "\n",
    "As before, this step may take a while to run (approx. 30 min), so we have prepared a precomputed object.\n",
    "To use it, **skip to step** `1.7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e4839-805e-4309-a931-b5d6321868de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%slurm_exec -i inf_aver,adata -o mod --time=00:60:00 --partition=gpu --gpus=1 --cpus=2 --mem=20G\n",
    "\n",
    "cell2location.models.Cell2location.setup_anndata(\n",
    "    adata=adata,\n",
    "    batch_key=\"sample\"\n",
    ")\n",
    "\n",
    "mod = cell2location.models.Cell2location(\n",
    "    adata,\n",
    "    cell_state_df=inf_aver,\n",
    "    N_cells_per_location=20,\n",
    "    detection_alpha=20\n",
    ")\n",
    "\n",
    "mod.train(\n",
    "    max_epochs=20000,\n",
    "    batch_size=None,\n",
    "    train_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3edebb-0c36-44ac-a60a-bd5ec389532b",
   "metadata": {},
   "source": [
    "Next, we extract relevant model statistics and store them in anndata reference object. \n",
    "\n",
    "As before, this step may take a while to run (approx 1 min), so we have prepared a precomputed object.\n",
    "To use it, **skip to step** `1.7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad93542b-4973-4fa2-9433-2c544aba448f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%slurm_exec -i adata,mod -o adata,mod --time=00:20:00 --partition=gpu --gpus=1 --cpus=2 --mem=20G\n",
    "\n",
    "adata = mod.export_posterior(\n",
    "    adata,\n",
    "    sample_kwargs={\n",
    "        'num_samples': 1000,\n",
    "        'batch_size': mod.adata.n_obs\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272bc0a1-45be-41ba-ade7-fb20cf16a516",
   "metadata": {},
   "source": [
    "### 1.7 Saving and loading model\n",
    "\n",
    "As in previous step, you can save your own model and load a pre-computed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379c908-fe38-454a-bb6b-a5675da5d8c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mod.save(f\"{OUTPUT_FOLDERNAME}/deconvolution\", overwrite=False)\n",
    "adata.write(f\"{OUTPUT_FOLDERNAME}/day1_with_cell2location.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3fb9b-ea9c-4beb-b230-c32bdec98b44",
   "metadata": {},
   "source": [
    "If you ever need to reload your own model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88846e34-3a2b-4b6c-83e0-7796576e9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(os.path.join(f\"{OUTPUT_FOLDERNAME}/deconvolution/\", 'day1_with_cell2location.h5ad'))\n",
    "mod = cell2location.models.Cell2location.load(\n",
    "    f\"{OUTPUT_FOLDERNAME}/cell2location/deconvolution\",\n",
    "    adata\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c1f40-86a7-4f19-aa45-83a8ceab705a",
   "metadata": {
    "tags": []
   },
   "source": [
    "As earlier, if you trained your own, **skip this step**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe09d3-2c81-4f8b-ba75-40471774d566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(os.path.join(f\"{PRECOMPUTED_FOLDERNAME}/cell2location/\", 'adata_with_cell2location.h5ad'))\n",
    "mod = cell2location.models.Cell2location.load(\n",
    "    f\"{PRECOMPUTED_FOLDERNAME}/cell2location/deconvolution\",\n",
    "    adata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81dde4-a84d-4831-9c6e-81a1590f3860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b153ba9-5a6f-481a-bffc-ca0735c6e43d",
   "metadata": {},
   "source": [
    "### 1.8 Visualising cell type abundances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4b358-836c-477a-ab85-48020bb9e7d6",
   "metadata": {},
   "source": [
    "Now lets inspect the results.\n",
    "We can take a look at the cell type estimate table for each spot here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c60ea2-81a4-413d-8171-2f555a0a4fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.obsm['q05_cell_abundance_w_sf']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3534a-ff64-4d82-b3a3-9141abde1099",
   "metadata": {},
   "source": [
    "And we can also add cell type abundance estimates to the meta data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d086a-31e1-457e-90ae-320d42a27129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.obs[adata.uns['mod']['factor_names']] = adata.obsm['q05_cell_abundance_w_sf']\n",
    "adata.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc08349b-3e85-4447-868a-9d0c1bef025b",
   "metadata": {},
   "source": [
    "We can now use the predicted abundances for plotting as before.\n",
    "\n",
    "Note that in these plots, we clipped the upper abundance range to enhance the visual contrast a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d28556-531b-428a-aec2-ea83b05266b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.pl.spatial_scatter(\n",
    "    adata,\n",
    "    cmap='viridis',\n",
    "    color=['T-Cells'],\n",
    "    vmax=3\n",
    ")\n",
    "sq.pl.spatial_scatter(\n",
    "    adata,\n",
    "    cmap='viridis',\n",
    "    color=['Fibroblasts'],\n",
    "    vmax=3\n",
    ")\n",
    "sq.pl.spatial_scatter(\n",
    "    adata,\n",
    "    cmap='viridis',\n",
    "    color=['Enterocytes'],\n",
    "    vmax=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe7972-2319-4146-8601-c7c18435e364",
   "metadata": {},
   "source": [
    "We can also use [cell2location](https://github.com/BayraktarLab/cell2location)'s blended plot function to visualise co-localisation of multiple cell types at once, although this gets visually messy if attempting to look at more than one cell type.\n",
    "\n",
    "Play around with different visualisations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5afe37-fff5-4850-9465-3d98956576dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clust_labels = ['T-Cells', 'Fibroblasts', 'Enterocytes']\n",
    "\n",
    "clust_col = ['' + str(i) for i in clust_labels] \n",
    "\n",
    "with mpl.rc_context({'figure.figsize': (15, 15)}):\n",
    "    fig = cell2location.plt.plot_spatial(\n",
    "        adata=adata,\n",
    "        color=clust_col, \n",
    "        labels=clust_labels,\n",
    "        show_img=True,\n",
    "        style='fast',\n",
    "        max_color_quantile=0.9,\n",
    "        circle_diameter=6,\n",
    "        colorbar_position='right'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a31057-daa4-405f-9cac-52267e583ff3",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can visualise the predicted abundance of each cell type per cluster using violin plots, which can be a bit clearer.\n",
    "\n",
    "For example, we can see that in our original clusters, T-Cells are highest in cluster 4. \n",
    "\n",
    "> **Exercise:**\n",
    "How does this compare to our other clustering solutions with spatial approaches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ecfe25-db3d-42dd-8297-eb797b2e3c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.violin(\n",
    "    adata,\n",
    "    keys=\"T-Cells\",\n",
    "    groupby=\"clusters\",\n",
    "    stripplot=False,\n",
    "    jitter=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab668402-1497-4300-b68f-34bd94c4b1f2",
   "metadata": {},
   "source": [
    "We can also calculate (scaled) mean cell type abundance per cluster and visualise as heatmap.\n",
    "This can be a quick way of understanding which cluster/region various cell types are most enriched in at a glance.\n",
    "\n",
    "For example, we can see that in additional to T-Cells, cluster 4 also has high abundance of macrophages, B-cells and plasma cells, as would be expected from immune follicles.\n",
    "Myofibroblasts and fibroblasts are also co-localising - we would expect this to be the case outside of epithelial regions, but it's also worth bearing in mind that these two cell types share a lot of gene expression programs, and it's possible co-localisation in these cases is an inaccuracy of deconvolution. Always be critical of your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e6e24-c262-4267-8490-6aea16a346d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cell_types = adata.uns[\"mod\"][\"factor_names\"]\n",
    "\n",
    "ab = adata.obs[cell_types]\n",
    "\n",
    "ab_cluster = ab.groupby(adata.obs[\"clusters\"]).mean()\n",
    "ab_cluster_z = ab_cluster.apply(\n",
    "    zscore,\n",
    "    axis=0\n",
    ")\n",
    "cmap = sns.diverging_palette(\n",
    "    h_neg=220,\n",
    "    h_pos=10,\n",
    "    as_cmap=True\n",
    ")\n",
    "\n",
    "sns.clustermap(\n",
    "    ab_cluster_z,\n",
    "    cmap=cmap,\n",
    "    center=0,\n",
    "    linewidths=0,\n",
    "    rasterized=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568a8f9-d0d8-4d5a-bffe-4c19120dbec8",
   "metadata": {},
   "source": [
    "We can also do a quick, basic cell type co-localisation check by simply checking the correlations between predicted cell type counts and plotting the correlation matrix as below.\n",
    "We can see here that we have some correlations that make sense - for example, transit amplifying cells and stem cells, both which we know should be located at the base of the crypt, have a strong co-localisation.\n",
    "Predicted B-cell localisation here is less biologically interpretable, which hints that our cell type deconvolution may not have worked accurately in all cases.\n",
    "\n",
    "You should always examine these results skeptically!\n",
    "Cell type deconvolution is not a \"solved\" problem in spatial transcriptomics, and methods that work well for certain tissues can struggle in others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb586ca-c257-4be0-85d4-f29b556e7efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr = adata.obs[cell_types].corr()\n",
    "cmap = sns.diverging_palette(\n",
    "    h_neg=220,\n",
    "    h_pos=10,\n",
    "    as_cmap=True\n",
    ")\n",
    "sns.clustermap(\n",
    "    corr,\n",
    "    cmap=cmap,\n",
    "    center=0,\n",
    "    square=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703224d5-1017-40e1-a0a0-0515f7618c49",
   "metadata": {},
   "source": [
    "Lets save the [anndata](https://anndata.readthedocs.io/en/stable/) with cell type predictions so we can easily load it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ba1ee-be43-4176-9781-1debfa436bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata_file = f\"{OUTPUT_FOLDERNAME}/adata_with_cell2location.h5ad\"\n",
    "adata.write(adata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23add915-c249-4853-b010-7eb7d7334f76",
   "metadata": {},
   "source": [
    "## 2. Spatial Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714bf462-95d9-4def-bc39-a834006a9091",
   "metadata": {},
   "source": [
    "Correlation analyses allow us to look at the signatures in the same spots, but what if we want to ask more precise questions about tissue areas that are adjacent to each other?\n",
    "We can do quite a bit of with spatial neighbour analyses. \n",
    "\n",
    "Analogously to a nearest neighbour graph in single-cell analysis, where neighbours are based on transcriptional similarity to each other, we can construct a spatial nearest neighbour graph, where neighbourhood is simply defined by physical space.\n",
    "We can then use this graph to look at co-localisation. \n",
    "\n",
    "We can construct this using [squidpy](https://squidpy.readthedocs.io/en/stable/) `spatial_neighbors()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b761056-d415-4556-bc26-a0fc0ce1d78c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.gr.spatial_neighbors(adata)\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c43e8-c3fe-4aac-b9cf-0bfbec33d484",
   "metadata": {},
   "source": [
    "[squidpy](https://squidpy.readthedocs.io/en/stable/) has several in-built functions for further analysis.\n",
    "\n",
    "For example, we might want to know which of our clusters are in close physical proximity.\n",
    "We can now use the neighbourhood enrichment test to visualise this.\n",
    "We can see that this identifies distinct domains, where clusters 1 and 10 co-localise in the outer \"roll\" of the tissue, while 0 and 3 in the middle layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d891ff-0415-47a5-960f-d087839db1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.gr.nhood_enrichment(\n",
    "    adata,\n",
    "    cluster_key=\"clusters\"\n",
    ")\n",
    "sq.pl.nhood_enrichment(\n",
    "    adata,\n",
    "    cluster_key=\"clusters\",\n",
    "    method=\"average\",\n",
    "    figsize=(5, 5)\n",
    ")\n",
    "sq.pl.spatial_scatter(\n",
    "    adata,\n",
    "    color=\"clusters\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c4c31-aa4c-4ee0-8bcf-628ccc6af19e",
   "metadata": {},
   "source": [
    "Previous cluster co-occurence analysis tested whether there is enrichment in the local neighbourhood.\n",
    "\n",
    "We can extend this question further and ask at what distance do clusters have strongest co-localisation.\n",
    "For this, we can calculate a co-occurance score across increasing distances.\n",
    "\n",
    "For example here if we select cluster 0 as a \"viewpoint\", we can see that the co-occurence with cluster 3 is strongest at shorter distances and then decreases.\n",
    "Co-occurence with cluster 2 is lower at low distances, but peaks at mid distances. \n",
    "\n",
    "Note that the distances here will always be distances calculated from whatever coordinate system your data is in.\n",
    "This could be pixels, or more meaningful microns - e.g. in the case of Xenium data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b307a28-a7e2-4eb1-ae59-7dce48d16b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.gr.co_occurrence(\n",
    "    adata,\n",
    "    cluster_key=\"clusters\"\n",
    ")\n",
    "sq.pl.co_occurrence(\n",
    "    adata,\n",
    "    cluster_key=\"clusters\",\n",
    "    clusters=\"0\",\n",
    "    figsize=(8, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033ccdd-e4ab-42df-9fa0-6d9af2805839",
   "metadata": {},
   "source": [
    "In spatial data, we can also do distance-based analyses.\n",
    "[Squidpy](https://squidpy.readthedocs.io/en/stable/) has a helper function that will allow us to calculate distances between spots and any selected feature. \n",
    "\n",
    "Here, for example, let's try to calculate distance of all spots to the nearest cluster 4 spot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006da645-07d1-4c0e-bf4a-1b44a86d2954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.tl.var_by_distance(\n",
    "    adata=adata,\n",
    "    groups=\"4\",\n",
    "    cluster_key=\"clusters\",\n",
    "    design_matrix_key=\"distance_to_cluster_4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa98808-20ca-40be-8e03-0d8c2cd5797d",
   "metadata": {},
   "source": [
    "The results table is stored here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61566a16-e3e6-4afa-a575-dc2a7890d2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.obsm[\"distance_to_cluster_4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dfef2f-b3d0-4623-9249-6bf76ec0e248",
   "metadata": {},
   "source": [
    "Lets add the distance as a column to obs for easier plotting and visualise the calculated distances.\n",
    "We can see that that we get a nice, smooth \"halo\" of spot distances to the nearest cluster 4 spot/follicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea620ec-45b7-47a9-ad54-2b27e3b048cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.obs[\"dist_cl4\"] = adata.obsm[\"distance_to_cluster_4\"][\"4_raw\"]\n",
    "sq.pl.spatial_scatter(adata, color=\"dist_cl4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37177c-04af-48e5-ade0-9e4e529dfb22",
   "metadata": {},
   "source": [
    "Now that we have calculated a distance of interest, we can also visualise how gene expression varies across that distance.\n",
    "\n",
    "For example, here we can see that Cd74 gene is highly expressed close to cluster 4 spots and decreases the further we go away from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f7976-9463-4cb3-b142-3874ac7d65a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.pl.var_by_distance(\n",
    "    adata=adata,\n",
    "    design_matrix_key=\"distance_to_cluster_4\",\n",
    "    var=[\"Cd74\"],\n",
    "    anchor_key=\"4\",\n",
    "    show_scatter=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aae347-c2ed-4112-9efc-f0daa5a9165f",
   "metadata": {},
   "source": [
    "For more specific analyses, we might want to select spots based on their adjacently to other features.\n",
    "For example, we might be interested in whether the spots immediatly surrounding lymphoid structures express any interesting genes. \n",
    "\n",
    "To answer a question like that, we need to be able to identify and select spots based on their proximity to other spots.\n",
    "We can use the distances we calculated earlier by thresholding spots based on distance - here, since cluster 4 were our spots of interest, the distance to nearest cluster 4 spot is zero, so we assign those as our \"Spots of Interest\" group, and then threshold other spots based on a small distance that will pick up only the first \"layer\" of adjacent spots. \n",
    "\n",
    "Then, we can make a new variable in the meta data of our anndata object with our groupings and visualise it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed63cc8-7f9e-4f62-a86b-4f23a36bce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the new column\n",
    "adata.obs[\"spot_group\"] = \"Other Spots\"\n",
    "# Identify the spots of interest\n",
    "adata.obs.loc[adata.obs[\"dist_cl4\"] == 0, \"spot_group\"] = \"Spots of Interest\"\n",
    "# Identify spots adjacents to the spots of interest\n",
    "adata.obs.loc[(adata.obs[\"dist_cl4\"] > 0) & (adata.obs[\"dist_cl4\"] <= 30), \"spot_group\"] = \"Adjacent Spots\"\n",
    "# Convert to categorical data type\n",
    "adata.obs[\"spot_group\"] = adata.obs[\"spot_group\"].astype(\"category\")\n",
    "# Plot\n",
    "sq.pl.spatial_scatter(adata, color=\"spot_group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac12d44-28ba-4610-a153-4d4dbb302694",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we can run marker detection between groups to identify genes of interest.\n",
    "\n",
    "Remember: we replaced our previous matrix with raw counts for cell type deconvolution analysis and so we should normalise our data again. \n",
    "\n",
    "**NOTE:** we also filtered out a whole bunch of uninformative low expression genes, so if you're interested in these, you should reload an earlier [AnnData](https://anndata.readthedocs.io/en/stable/) object for this instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039afee-abfd-4e7c-89e7-a1acfc323176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(\n",
    "    adata,\n",
    "    inplace=True\n",
    ")\n",
    "sc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f122dd1e-cbb8-4435-9e9e-e80865bc718a",
   "metadata": {},
   "source": [
    "Now, we can run a marker search between selected and adjacent tissue spots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a59cc9-16ea-4d18-9a10-75993d520d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.obs[\"spot_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf8efd-91ee-4516-ba23-6b554359cd1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(\n",
    "    adata,\n",
    "    groupby=\"spot_group\",\n",
    "    method=\"wilcoxon\",\n",
    "    use_raw=False,\n",
    "    groups=[\"Adjacent Spots\"],\n",
    "    reference=\"Spots of Interest\"\n",
    ")\n",
    "sc.get.rank_genes_groups_df(\n",
    "    adata,\n",
    "    group=\"Adjacent Spots\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9bc21-9443-4a43-b8af-d9542e674838",
   "metadata": {},
   "source": [
    "For example, we can see that *Cd74* is higher within the follicles (spots of interest), while *Atp1a1* is lower. \n",
    "\n",
    "We can also compare cell type deconvolution results.\n",
    "Again, we can see that T-cells are more abundant in our spots of interest, but not in the layer immediately outside.\n",
    "\n",
    "You could also compare adjacent spots with all other spots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9140aa-0138-4d9c-a98e-ca46fcf1dc82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.violin(\n",
    "    adata,\n",
    "    keys=[\"Cd74\", \"Atp1a1\"],\n",
    "    groupby=\"spot_group\",\n",
    "    stripplot=False,\n",
    "    jitter=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193e732-738c-409f-9777-30ee958f6c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.violin(\n",
    "    adata,\n",
    "    keys=[\"T-Cells\", \"Macrophages\"],\n",
    "    groupby=\"spot_group\",\n",
    "    stripplot=False,\n",
    "    jitter=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669bd464-8cb1-4dbb-9963-9f57f8d0c24b",
   "metadata": {},
   "source": [
    "## 3. Working with multiple samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f0c682-050c-4bbf-8dbc-a5442e5d9eb5",
   "metadata": {},
   "source": [
    "Typically, you will not be working with just one sample but multiple ones across different conditions.\n",
    "We will next discuss how to work with multiple slices. \n",
    "\n",
    "Here, we have prepared a second [AnnData](https://anndata.readthedocs.io/en/stable/) object from \"DAY14 DSS\" treated mouse intestine, which has been pre-processed through the exact same steps as the DAY0 healthy tissue section that we used during day 1.\n",
    "\n",
    "**NOTE:** You can also do this yourself, by going through yesterday's workflow using the [spaceranger](https://www.10xgenomics.com/support/software/space-ranger/latest) outputs for this sample available here:\n",
    "\n",
    "```\n",
    "/project/shared/r/3_r_spatial/DATA/VISIUM_V1_MOUSE_INTESTINE/spaceranger/SRR14083627_DSS_DAY14\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4e2f7-bc11-4f63-b23b-e78b36a4a319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "section1 = sc.read_h5ad(os.path.join(PRECOMPUTED_FOLDERNAME, 'day1.h5ad'))\n",
    "section2 = sc.read_h5ad(os.path.join(PRECOMPUTED_FOLDERNAME, 'day14_precomputed.h5ad'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e11079-cb58-41d6-9f73-6cff71efaf27",
   "metadata": {},
   "source": [
    "Let's inspect both tissue sections.\n",
    "We have identified clusters on both slides, but they are individual section clusters and therefore not the same between slides.\n",
    "For downstream analysis, we need a unified grouping, such that cluster 1 in control slide is the same as cluster 1 in treatment section if we want to do any comparisons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037e61b-5fc2-4639-b74c-42a237a3d3fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.pl.spatial_scatter(\n",
    "    section1,\n",
    "    color=\"clusters\"\n",
    ")\n",
    "sq.pl.spatial_scatter(\n",
    "    section2,\n",
    "    color=\"clusters\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42a89e-7715-45e6-8762-d3e9ef63a322",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Merging datsets\n",
    "\n",
    "The `concat()` function in [scanpy](https://scanpy.readthedocs.io/en/stable/) is used to combine multiple objects into a single object. \n",
    "\n",
    "This is particularly useful when you have data from different sections, conditions or batches that you want to analyse together. \n",
    "\n",
    "In this case, the code merges two [AnnData](https://scanpy.readthedocs.io/en/stable/) objects (e.g., from different tissue sections) into one combined [AnnData](https://scanpy.readthedocs.io/en/stable/) object.\n",
    "\n",
    "This should also merge your `spatial` and `image` slots such that each image is retained and accessible for plotting.\n",
    "\n",
    "**TIP:**\n",
    "Basic meta data has been added to individual section objects already.\n",
    "To keep the meta data tidy, ensure the meta data columns across all objects to be merged are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06712d4-cbe9-4e58-b156-e894b412f861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged = sc.concat(\n",
    "    [\n",
    "        section1,\n",
    "        section2\n",
    "    ],\n",
    "    label=\"dataset\",\n",
    "    uns_merge=\"unique\",\n",
    "    keys=[\n",
    "        'Day0',\n",
    "        'Day14'\n",
    "    ],\n",
    "    index_unique=\"-\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f7c0b9-9bea-456e-b159-0e55e3a0b264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab4bba-fd79-4591-934c-58f4a0d826e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182b9c7-fab6-4108-89b0-c32d0ed95c34",
   "metadata": {},
   "source": [
    "We can do a sanity check to see how many spots there are in the object in each tissue section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb11a98-1e82-438d-8b8e-29356bb35218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged.obs['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd1cf83-6bb2-4b34-b577-ee917109912c",
   "metadata": {},
   "source": [
    "To analyse merge object data, generally we want to repeat the basic workflow we carried out for one sample in day 1, except now do this for the merged object.\n",
    "As the data were previously normalised, we don't need to do those steps again.\n",
    "\n",
    "**We want to start from variable gene selection.**\n",
    "One key difference is that when we calculate highly variable genes for multiple samples, it is important that gene selection is performed in a *batch-aware* way.\n",
    "This is because genes that are variable across the whole dataset could be capturing batch effects rather than the biological signals we are interested in. \n",
    "\n",
    "We can perform batch-aware highly variable gene selection by setting the `batch_key=` argument in the [scanpy](https://scanpy.readthedocs.io/en/stable/) `highly_variable_genes()` function.\n",
    "[scanpy](https://scanpy.readthedocs.io/en/stable/) will then calculate HVGs for each batch separately and combine the results by selecting those genes that are highly variable in the highest number of batches. \n",
    "\n",
    "We use the [scanpy](https://scanpy.readthedocs.io/en/stable/) function here because it has this batch awareness built in.\n",
    "For other methods, we would have to run them on each batch individually and then manually combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e625b09-fcef-4950-a7b9-1ae715a5ccf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(\n",
    "    merged,\n",
    "    n_top_genes=2000,\n",
    "    flavor=\"seurat\",\n",
    "    batch_key='dataset'\n",
    ")\n",
    "merged.var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb64bce-89b0-4603-993d-08d75f90febf",
   "metadata": {},
   "source": [
    "Next, we repeat the PCA and clustering analysis as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d4011-6a1c-4287-ab89-2f40a75651f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.pca(merged)\n",
    "sc.pp.neighbors(\n",
    "    merged,\n",
    "    n_pcs=10\n",
    ")\n",
    "sc.tl.umap(merged)\n",
    "sc.tl.leiden(\n",
    "    merged,\n",
    "    key_added=\"cluster\",\n",
    "    flavor=\"igraph\",\n",
    "    n_iterations=2,\n",
    "    resolution=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb0142-3308-40fd-b96e-913f7b940877",
   "metadata": {},
   "source": [
    "We can visualise the results over the UMAP embedding.\n",
    "However, when we check the clusters, we can see that spots from different slides form their own clusters.\n",
    "We don't want this, but rather to find joint regions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bf025-aa94-4d1b-8c27-17c88fb6545a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    merged,\n",
    "    color=['dataset', 'cluster']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba445a5-ef52-4d8b-949f-9c9743fa1ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.pl.spatial_scatter(\n",
    "    merged,\n",
    "    color=\"cluster\",\n",
    "    library_key=\"dataset\",\n",
    "    library_id=[\"Day0\", \"Day14\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906b063c-6d03-4f95-b81e-df5da1003177",
   "metadata": {},
   "source": [
    "[harmony](https://github.com/immunogenomics/harmony) is an algorithm designed for integrating single-cell data from different batches or conditions while preserving biological variability.\n",
    "It is particularly effective in handling batch effects and other sources of technical variation.\n",
    "We can generally use most methods designed for single-cell analysis on spatial transcriptomics data integration - e.g. another good alternative is [scVI](https://scvi-tools.org/). \n",
    "\n",
    "[harmony](https://github.com/immunogenomics/harmony) works by correcting reduced dimensions between datasets.\n",
    "In most use cases, this corrects the standard PCA matrix derived from gene expression.\n",
    "However, we can use [harmony](https://github.com/immunogenomics/harmony) on any dimensionality reduction - this means that, for example, you could substitute this with the features/PCA calculated using [Banksy](https://github.com/prabhakarlab/Banksy) algorithm to use spatial clustering with [harmony](https://github.com/immunogenomics/harmony) integration together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed759032-37e6-41d6-86f1-033c3efc4768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.external.pp.harmony_integrate(\n",
    "    merged,\n",
    "    key='dataset',\n",
    "    basis='X_pca',\n",
    "    adjusted_basis='X_pca_harmony'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d118cf-2f36-4180-949c-8f10d37f2d23",
   "metadata": {},
   "source": [
    "Once we have harmonised PCs, we can now use these as input for clustering and umap instead of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7c9d1-a45e-48ef-a030-e6a60412dd91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.neighbors(\n",
    "    merged,\n",
    "    n_pcs=10,\n",
    "    use_rep=\"X_pca_harmony\"\n",
    ")\n",
    "sc.tl.umap(merged)\n",
    "sc.tl.leiden(\n",
    "    merged,\n",
    "    resolution=0.5,\n",
    "    key_added=\"harmony_cluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf8771-91b0-491c-afbf-338affba2bd2",
   "metadata": {},
   "source": [
    "We can see that we now have clusters where the two tissue sections are aligned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e76b1-877c-45b4-8b11-97218f5f8687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    merged,\n",
    "    color=['dataset','harmony_cluster']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d9246b-058a-4b2c-a40b-a644bec8aa66",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can now see that harmonised clusters appear to be consistent between two tissue sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6b919-96bf-4adf-81bb-0e15a59f367e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.pl.spatial_scatter(\n",
    "    merged,\n",
    "    color=\"harmony_cluster\",\n",
    "    library_key=\"dataset\",\n",
    "    library_id=[\"Day0\", \"Day14\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b0f47-56ed-40f5-8103-b8aa3ba08dc9",
   "metadata": {},
   "source": [
    "Let's have a quick look to see if we can find some differences between datasets.\n",
    "Since we only have two sections, we can still use wilcoxon test to compare them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5bc24-36a8-4357-9ed3-9b77b21fd6c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(\n",
    "    merged,\n",
    "    groupby=\"dataset\",\n",
    "    method=\"wilcoxon\",\n",
    "    use_raw=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db823b12-3bef-4f73-9adb-bf74fba9ddf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.get.rank_genes_groups_df(\n",
    "    merged,\n",
    "    group=\"Day0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58c235-f321-4152-88c0-ee32344ad877",
   "metadata": {},
   "source": [
    "As an example, can see one differentially expressed gene, *Krt13*, has been detected in the DSS-treated tissue sample but not in the control.\n",
    "We did a simple whole slide level comparison, but we could also modify the above to ask more specific questions about differences within certain common regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba83b628-62fa-41eb-a659-0282702931cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.pl.spatial_scatter(\n",
    "    merged,\n",
    "    color=\"Krt13\",\n",
    "    library_key=\"dataset\",\n",
    "    library_id=[\"Day0\", \"Day14\"],\n",
    "    vmin=0,\n",
    "    vmax=merged[:, \"Krt13\"].X.max()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a6c3b-93bf-472d-abb8-e955bac07130",
   "metadata": {},
   "source": [
    "Let's save the merged anndata object at the end of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc0bc3-220b-4ad4-9a14-d55fb48572ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write(os.path.join(OUTPUT_FOLDERNAME, f'day2_merged.h5ad'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb6e06-b3a4-43f6-b071-5e9b779326c1",
   "metadata": {},
   "source": [
    "## 4. Working with HD data at multiple resolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94212e8-befd-4e2e-9a5c-002b1c59263f",
   "metadata": {},
   "source": [
    "### Higher resolution datsets - Visium HD\n",
    "\n",
    "Finally, we will briefly explore how higher resolution sequencing-based datasets compare to more common spot-based datasets. \n",
    "\n",
    "Generally, due to increased resolution, this analysis takes much, **much**, longer and requires more computational resources. \n",
    "\n",
    "Here, we will use a mouse intestine [Visium HD](https://www.10xgenomics.com/platforms/visium/product-family) dataset, which will enable us to compare (at least qualitatively) the differences between two technologies as the tissues are very similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac92e9-0998-493a-9230-d766b87e85f4",
   "metadata": {},
   "source": [
    "First, we want to load the data.\n",
    "\n",
    "Previously, we have used [squidpy](https://squidpy.readthedocs.io/en/stable/) to create [AnnData](https://anndata.readthedocs.io/en/stable/) objects from our spatial data.\n",
    "Here, we will use [SpatialData](https://spatialdata.scverse.org/en/stable/) data structure instead, as [squidpy](https://squidpy.readthedocs.io/en/stable/) does not support the new HD data formats. \n",
    "\n",
    "You can check the [SpatialData](https://spatialdata.scverse.org/en/stable/) documentation for additional information on the data structures: \n",
    "\n",
    "https://spatialdata.scverse.org/en/latest/tutorials/notebooks/notebooks/examples/intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e136ca0-8139-4826-9343-e6d60343eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{DATA_FOLDERNAME}/visium_hd_mouse_intestine\"\n",
    "hd = visium_hd(path, dataset_id='Visium_HD_Mouse_Small_Intestine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f29772-beb5-4fe6-bc4f-9c8ba405ec98",
   "metadata": {},
   "source": [
    "If we inspect the data object `hd`, you can see that we have ingested three main parts:\n",
    "\n",
    "**images**,\n",
    "which contain various resolutions of our H&E images\n",
    "\n",
    "**shapes**,\n",
    "which in other spatial datasets would contain cell segmentation polygons (but here we keep spatial bins)\n",
    "\n",
    "and finally there are three **tables**.\n",
    "\n",
    "The tables are stored as [AnnData](https://anndata.readthedocs.io/en/stable/) objects and are analogous to [squidpy](https://squidpy.readthedocs.io/en/stable/).\n",
    "\n",
    "We have three tables, where bins have been summarised at different resolutions.\n",
    "As you can hopefully appreciate, the highest resolution bin ingests more than 5 million data points, which could be very computationally intense to work with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f034c93a-8b3f-4d60-a0a9-cb9ae470add5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1c328-c71a-44e7-8f33-6b567280a143",
   "metadata": {},
   "source": [
    "We can access individual anndata objects and operate on them as before with standard scanpy/squidpy functions and workflows. For example, we can inspect the 16um bin anndata object here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29440cf5-37ed-45f7-91f1-f481d2983c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hd['square_016um']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6af1b9-cf87-4168-a732-3446a71add2a",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, we will just carry out some very basic filtering on this dataset to remove spots with fewer than 200 counts detected.\n",
    "\n",
    "In an ideal workflow, you would explore the QC metrics as we did in day 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50654f0c-e719-46d4-a096-fd849046d8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.filter_cells(\n",
    "    hd['square_016um'],\n",
    "    min_counts=200,\n",
    "    inplace=True\n",
    ")\n",
    "hd['square_016um']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9091d-ff48-4553-8b3b-a3381cf560f7",
   "metadata": {},
   "source": [
    "Next, lets run some quick normalisation, PCA and clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b9111-7b4c-48dd-9bfe-03eceffcf8e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(\n",
    "    hd['square_016um'],\n",
    "    inplace=True\n",
    ")\n",
    "sc.pp.log1p(\n",
    "    hd['square_016um']\n",
    ")\n",
    "sc.pp.highly_variable_genes(\n",
    "    hd['square_016um'],\n",
    "    flavor=\"seurat\",\n",
    "    n_top_genes=2000\n",
    ")\n",
    "sc.pp.pca(\n",
    "    hd['square_016um']\n",
    ")\n",
    "sc.pp.neighbors(\n",
    "    hd['square_016um'],\n",
    "    n_pcs=20\n",
    ")\n",
    "sc.tl.umap(\n",
    "    hd['square_016um']\n",
    ")\n",
    "sc.tl.leiden(\n",
    "    hd['square_016um'],\n",
    "    key_added=\"clusters\",\n",
    "    flavor=\"igraph\",\n",
    "    n_iterations=2,\n",
    "    resolution=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a4934-80a4-410e-9aac-350c9c891b35",
   "metadata": {},
   "source": [
    "We can visualise the results with `umap()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d659fe-524f-4073-84ed-8bb57069ee4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    hd['square_016um'],\n",
    "    color=[\"clusters\"],\n",
    "    wspace=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864aaa26-d821-4ce6-8924-768530d9ef44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hd['square_016um']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35acf04d-8c84-40dc-96f3-714825b4ee71",
   "metadata": {},
   "source": [
    "And we can use spatial scatter from [squidpy](https://squidpy.readthedocs.io/en/stable/) to visualise clusters.\n",
    "Already, you can hopefully see that even that lowest resolution of bins gives us a much more detailed view of the mouse intestine than Visium V1 data in this swiss roll. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d046534-f790-4205-b5f0-60506b5c9f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.pl.spatial_scatter(\n",
    "    hd['square_016um'],\n",
    "    shape=None,\n",
    "    color=\"clusters\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ec602-d3f9-4d18-b67e-a3de995a6b53",
   "metadata": {},
   "source": [
    "Now lets repeat the analysis on higher resolution bins.\n",
    "Again, we will do some very basic filtering to remove some spots, but in practice you would want to explore all QC metrics here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b808bd-dcdf-440c-bf97-6c7e5b8642ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.filter_cells(\n",
    "    hd['square_008um'],\n",
    "    min_counts=200,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f4487-0cba-4687-a027-82fd19a33f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(\n",
    "    hd['square_008um'],\n",
    "    inplace=True\n",
    ")\n",
    "sc.pp.log1p(\n",
    "    hd['square_008um']\n",
    ")\n",
    "sc.pp.highly_variable_genes(\n",
    "    hd['square_008um'],\n",
    "    flavor=\"seurat\",\n",
    "    n_top_genes=2000\n",
    ")\n",
    "sc.pp.pca(\n",
    "    hd['square_008um']\n",
    ")\n",
    "sc.pp.neighbors(\n",
    "    hd['square_008um'],\n",
    "    n_pcs=20\n",
    ")\n",
    "sc.tl.umap(\n",
    "    hd['square_008um']\n",
    ")\n",
    "sc.tl.leiden(\n",
    "    hd['square_008um'],\n",
    "    key_added=\"clusters\",\n",
    "    flavor=\"igraph\",\n",
    "    n_iterations=2,\n",
    "    resolution=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7db174-8cd4-4aa6-a53e-970fb1986e95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    hd['square_008um'],\n",
    "    color=[\"clusters\"],\n",
    "    wspace=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4593591-9ce6-4b7f-b551-1fd12d636afe",
   "metadata": {},
   "source": [
    "Here we can see that we get a lot more structure and detail in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69886b-ac30-4a65-bdb4-160d9d743e03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.pl.spatial_scatter(\n",
    "    hd['square_008um'],\n",
    "    shape=None,\n",
    "    color=\"clusters\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93203c74-bd10-4973-8517-be58295bd136",
   "metadata": {},
   "source": [
    "We can define a crop to zoom into a smaller region of the slide to compare clusters between resolutions in a bit more detailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da012c-1b37-4960-aad1-1fee7ba0891a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crop = (12000, 12000, 16000, 16000)\n",
    "sq.pl.spatial_scatter(\n",
    "    hd['square_016um'],\n",
    "    shape=None,\n",
    "    color=\"clusters\",\n",
    "    crop_coord=crop,\n",
    "    size=30\n",
    ")\n",
    "sq.pl.spatial_scatter(\n",
    "    hd['square_008um'],\n",
    "    shape=None,\n",
    "    color=\"clusters\",\n",
    "    crop_coord=crop,\n",
    "    size=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9386e2-668f-4b35-9fa5-87efcc7914e1",
   "metadata": {},
   "source": [
    "We can save spatial data objects to disk as [zarr](https://zarr.dev/) files to reload at a later time point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6baa44-8e09-44a3-8077-63d887960e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd.write(os.path.join(OUTPUT_FOLDERNAME, \"visium_hd.zarr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ed7e1-3b3d-43b2-bd35-f2c0f42df4bd",
   "metadata": {},
   "source": [
    "### Single Cell Segmentation of HD Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013811af-a7e9-49ef-bb23-b05d75381101",
   "metadata": {},
   "source": [
    "The above analysis uses bins as they are measured on the slide for analysis.\n",
    "But they generally do not correspond to cells.\n",
    "In the workflow below, we will go through the steps we can take to partition the data into cells instead.\n",
    "\n",
    "We will use a method called [bin2cell](https://github.com/Teichlab/bin2cell), which wraps [stardist](https://github.com/stardist/stardist) algorithm for cell and nuclei segmentation and provides functions for intersecting the outputs with spatial transcriptomics data.\n",
    "\n",
    "You can read more about the method here:\n",
    "\n",
    "https://academic.oup.com/bioinformatics/article/40/9/btae546/7754061\n",
    "\n",
    "More specifically on stardist:\n",
    "\n",
    "https://github.com/stardist/stardist\n",
    "\n",
    "NOTE: Latest version of [spaceranger](https://www.10xgenomics.com/support/software/space-ranger/latest) now implements are similar approach that we take here.\n",
    "In the data that we are using for this lesson, this was not yet available.\n",
    "In our approach, we will use a combination of nuclei-based segmentation and transcript density based segmentation, while [spaceranger](https://www.10xgenomics.com/support/software/space-ranger/latest) uses the nuclei only. \n",
    "\n",
    "There are multiple other high resolution spatial platforms that generate very high resolution bin data (e.g. stereo-seq) where cell level segmentation may not be generated automatically and you might need to apply this approach yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aca54e-3051-4152-9333-f457c9bb7f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = f\"{DATA_FOLDERNAME}/visium_hd_mouse_intestine/binned_outputs/square_002um/\"\n",
    "source_image_path = f\"{DATA_FOLDERNAME}/visium_hd_mouse_intestine/image.tiff\"\n",
    "spaceranger_image_path = f\"{DATA_FOLDERNAME}/visium_hd_mouse_intestine/spatial/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666a901-42d4-47e0-ba25-8a2217e5896d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata = b2c.read_visium(\n",
    "    path,\n",
    "    source_image_path = source_image_path,\n",
    "    spaceranger_image_path = spaceranger_image_path\n",
    ")\n",
    "adata.var_names_make_unique()\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c5120-88cb-43fa-aef3-9ea1157b7121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.calculate_qc_metrics(\n",
    "    adata,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21304a-7379-4a16-bae8-be7e5530d0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.filter_cells(\n",
    "    adata,\n",
    "    min_counts=1,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff275a6f-2e6d-493f-b197-bdb580efcb2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(OUTPUT_FOLDERNAME, \"stardist\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b748e-f4ae-49d8-a81f-87140ce98616",
   "metadata": {},
   "source": [
    "First we want to rescale the high resolution H&E image from visium to roughly the resolution that [stardist](https://github.com/stardist/stardist) nuclei detection model will have been trained on.\n",
    "We can use the scale function provided by [bin2cell](https://github.com/Teichlab/bin2cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e4a77-139b-4238-97ae-845c90d26e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpp = 0.3\n",
    "b2c.scaled_he_image(\n",
    "    adata,\n",
    "    mpp=mpp,\n",
    "    save_path=os.path.join(OUTPUT_FOLDERNAME, \"stardist\", \"he.tiff\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04850ca-5431-4367-8e47-78e4a61c3d2c",
   "metadata": {},
   "source": [
    "Next, because we are working with the smallest 2um bins that are measured on the slide, we (optionally) want to correct for any artefacts from uneven capture between bins, as the smallest bins are in practice slightly uneven, which causes small \"stripe\" artefacts in the data.\n",
    "This is generally not an issue in larger size bins we have been working with previously, as signal is aggregated across a larger area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae08939-0b7f-4f14-87d0-122a95a2c71b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b2c.destripe(\n",
    "    adata,\n",
    "    counts_key=\"total_counts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df86095-0d34-49d3-ab74-8fbcf44a12b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e9d9d-59bf-4ee2-b712-1b3b9c61d1d4",
   "metadata": {},
   "source": [
    "We then run [stardist](https://github.com/stardist/stardist), which detects nuclei in our H&E images.\n",
    "The below step might take a bit of time (~15 mins) to run, therefore **you can skip this step** and use our pre-computed segmentation in the next step. \n",
    "\n",
    "Lowering `prob_thresh=` to make the calls less stringent - you might want to tweak this to \"rescue\" more cells/nuclei, but it will also create more objects which might not be real cells. \n",
    "\n",
    "`nms_thresh=` parameter tells the segmentation what's the expected overlap between objects.\n",
    "Tweaking this can help resolve segmentation in areas which are densely packed, but can also over-partition a one cell in multiple objects.\n",
    "\n",
    "Expect the cell below to take around 12 min to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead09a08-4680-425d-800f-f44a06085f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%slurm_exec --time=00:20:00 --partition=gpu --gpus=1 --cpus=2 --mem=30G\n",
    "\n",
    "b2c.stardist(\n",
    "    image_path=os.path.join(OUTPUT_FOLDERNAME, \"stardist\", \"he.tiff\"),\n",
    "    labels_npz_path=os.path.join(OUTPUT_FOLDERNAME, \"stardist\", \"he.npz\"),\n",
    "    stardist_model=\"2D_versatile_he\",\n",
    "    prob_thresh=0.01,\n",
    "    nms_thresh=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b780fb75-0d0a-4404-a1f5-e90867770ab6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once nuclei detection is complete, we can read in the results and attach the segmentations to our anndata object. This step assigns individual bins to nuclei boundaries. \n",
    "\n",
    "**NOTE:**\n",
    "If you'd like to load pre-computed segmentation, the equivalent files are stored here:\n",
    "\n",
    "```\n",
    "/project/shared/python/5_python_spatial_omics/data/precomputed/day1/stardist\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6ef25-6d8e-48ac-afbc-563f04d04ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b2c.insert_labels(\n",
    "    adata,\n",
    "    labels_npz_path=os.path.join(OUTPUT_FOLDERNAME, \"stardist\", \"he.npz\"),\n",
    "    basis=\"spatial\",\n",
    "    spatial_key=\"spatial_cropped_150_buffer\",\n",
    "    mpp=mpp,\n",
    "    labels_key=\"labels_he\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c0428-1119-46c6-aa80-e294c4413790",
   "metadata": {},
   "source": [
    "Cells are not just nuclei.\n",
    "One approach we can take here is to expand the nuclei boundaries to include additional nearby spots that would be expected to fall in the cytoplasm.\n",
    "This will not be 100% accurate, as ideally you'd need to have a specific cell boundary image stain to infer the cell boundaries with more precision, but it's a good approach when we only have H&E images.\n",
    "\n",
    "`algorithm=\"volume_ratio\"` assigns each nucleus a custom expansion distance based on its bin count based on the expected ratio between cell and nucleus volume.\n",
    "If a bin is equidistant between two nuclei, its a assigned to a nucleus based on gene expression profile similarity.\n",
    "\n",
    "Here, you could alternatively set a fixed bin expansion instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ebfa0-5e42-47a7-b50c-4b0f5b34a82c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b2c.expand_labels(\n",
    "    adata,\n",
    "    labels_key='labels_he',\n",
    "    expanded_labels_key=\"labels_he_expanded\",\n",
    "    algorithm=\"volume_ratio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d4c766-ab59-4722-b658-3550485d9630",
   "metadata": {},
   "source": [
    "The images are very large and so it can be hard to visualise how individual spots are assigned to cells.\n",
    "Here, we pick a very small field of view to get a quick visual of how nuclei segmentation is working.  \n",
    "\n",
    "[bin2cell](https://github.com/Teichlab/bin2cell) bin labels are integers, where 0 means a bin is not assigned to any cell.\n",
    "Therefore, we also want to remove any non-cell bins for visual clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b8b3b-2f29-40bd-bae2-3b33ecf6f457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (adata.obs['array_row'] >= 1530) &\n",
    "    (adata.obs['array_row'] <= 1550) &\n",
    "    (adata.obs['array_col'] >= 390) &\n",
    "    (adata.obs['array_col'] <= 410)\n",
    ")\n",
    "\n",
    "adata_subset = adata[mask].copy()\n",
    "\n",
    "adata_subset = adata_subset[adata_subset.obs['labels_he']>0].copy()\n",
    "\n",
    "adata_subset.obs['labels_he'] = adata_subset.obs['labels_he'].astype('category')\n",
    "\n",
    "sc.pl.spatial(\n",
    "    adata_subset,\n",
    "    color=\"labels_he\",\n",
    "    img_key=\"0.3_mpp_150_buffer\",\n",
    "    basis=\"spatial_cropped_150_buffer\",\n",
    "    palette=\"tab20\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5043d0-079b-459f-a62e-07fe08d703cf",
   "metadata": {},
   "source": [
    "Nuclei detection can be unreliable, and we might have cells where nuclei are not picked up so well either by staining or segmentation.\n",
    "[bin2cell](https://github.com/Teichlab/bin2cell) allows us to (optionally) use gene expression density-based cell detection step on top of nuclei segmentation. \n",
    "\n",
    "To do this, we first need to create a gene expression intensity image from the bins.\n",
    "Here, we use adjusted, \"destriped\" UMI counts to obtain a more uniform signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a04c65-6efb-4ddb-9034-a1ddff94461a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b2c.grid_image(\n",
    "    adata,\n",
    "    \"n_counts_adjusted\",\n",
    "    mpp=mpp,\n",
    "    sigma=5,\n",
    "    log1p=True,\n",
    "    save_path=os.path.join(OUTPUT_FOLDERNAME, \"stardist\", \"gex.tiff\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9fdc0a-2327-4dd4-b53a-316551cf49e6",
   "metadata": {},
   "source": [
    "Then, as before, we can call [stardist](https://github.com/stardist/stardist), but this time instead of segmenting the image on H&E stain, we use the transcript density map instead.\n",
    "\n",
    "Expect the cell below to take around 3 min to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19fa84-35a8-45b2-92e3-8fbda5bf3267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%slurm_exec --time=00:20:00 --partition=gpu --gpus=1 --cpus=2 --mem=20G\n",
    "\n",
    "b2c.stardist(\n",
    "    image_path=os.path.join(OUTPUT_FOLDERNAME, \"stardist\", \"gex.tiff\"),\n",
    "    labels_npz_path=os.path.join(OUTPUT_FOLDERNAME, \"stardist\", \"gex.npz\"),\n",
    "    stardist_model=\"2D_versatile_fluo\",\n",
    "    prob_thresh=0.05,\n",
    "    nms_thresh=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228d973-74b4-4a68-977d-8d669a50d606",
   "metadata": {},
   "source": [
    "Read in the segmentations and add them to our [AnnData](https://anndata.readthedocs.io/en/stable/) object.\n",
    "\n",
    "**Note:**\n",
    "If you'd like to load pre-computed segmentation, the equivalent files are stored here:\n",
    "\n",
    "```\n",
    "/project/shared/python/5_python_spatial_omics/data/precomputed/day1/stardist\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640c980-266c-4f8c-b0ad-f22ce1a7598b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b2c.insert_labels(\n",
    "    adata,\n",
    "    labels_npz_path=os.path.join(OUTPUT_FOLDERNAME, \"stardist\", \"gex.npz\"),\n",
    "    basis=\"array\",\n",
    "    mpp=mpp,\n",
    "    labels_key=\"labels_gex\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2f844-b43e-41ac-9c16-45061c4db0fd",
   "metadata": {},
   "source": [
    "Lets visualise in the same spatial region as our nuclei-based segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa5f02-4d61-41d5-b228-647767f29787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata_subset = adata[mask].copy()\n",
    "\n",
    "adata_subset = adata_subset[adata_subset.obs['labels_gex']>0].copy()\n",
    "\n",
    "adata_subset.obs['labels_gex'] = adata_subset.obs['labels_gex'].astype('category')\n",
    "\n",
    "sc.pl.spatial(\n",
    "    adata_subset,\n",
    "    color=\"labels_gex\",\n",
    "    img_key=\"0.3_mpp_150_buffer\",\n",
    "    basis=\"spatial_cropped_150_buffer\",\n",
    "    palette=\"tab20\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4930fb4b-970e-4d06-a0b0-c94585cee295",
   "metadata": {},
   "source": [
    "We can also visualise actual cell boundary polygons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095a59d-7d3a-4b0a-9143-a48da64f01a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crop = b2c.get_crop(\n",
    "    adata_subset,\n",
    "    basis=\"array\",\n",
    "    mpp=mpp\n",
    ")\n",
    "\n",
    "rendered = b2c.view_labels(\n",
    "    image_path=os.path.join(OUTPUT_FOLDERNAME, \"stardist\", \"gex.tiff\"),\n",
    "    labels_npz_path=os.path.join(OUTPUT_FOLDERNAME, \"stardist\", \"gex.npz\"),\n",
    "    crop=crop,\n",
    "    stardist_normalize=True\n",
    ")\n",
    "plt.imshow(rendered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85712779-c05d-4438-9cf8-84e82f60ac25",
   "metadata": {},
   "source": [
    "For downstream data analysis, we can combine the nuclei based segmentation with UMI-based segmentation results to get joint labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b209b-6b4e-4aef-b8b5-76f3f1dc6c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b2c.salvage_secondary_labels(\n",
    "    adata,\n",
    "    primary_label=\"labels_he_expanded\",\n",
    "    secondary_label=\"labels_gex\",\n",
    "    labels_key=\"labels_joint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24dc5e-dfba-41a5-99f4-cf5044785fbd",
   "metadata": {},
   "source": [
    "At this point, we can generate a new [AnnData](https://anndata.readthedocs.io/en/stable/) object which aggregates the UMI counts per cell.\n",
    "We can see that we have a lot of cells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986078e7-e165-48cb-8c40-bd6ec61a0909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata_cell = b2c.bin_to_cell(\n",
    "    adata,\n",
    "    labels_key=\"labels_joint\",\n",
    "    spatial_keys=[\n",
    "        \"spatial\",\n",
    "        \"spatial_cropped_150_buffer\"\n",
    "    ]\n",
    ")\n",
    "adata_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e7e9e-89a8-4a96-8f82-801f956e49bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata_cell.var[\"mt\"] = adata_cell.var_names.str.startswith(\"mt-\")\n",
    "adata_cell.var[\"rb\"] = adata_cell.var_names.str.startswith(\"Rp\")\n",
    "sc.pp.calculate_qc_metrics(\n",
    "    adata_cell,\n",
    "    qc_vars=[\n",
    "        \"mt\",\n",
    "        \"rb\"\n",
    "    ],\n",
    "    inplace=True\n",
    ")\n",
    "adata_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed85d4-f4c5-48f0-a94d-e72d37a9dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    adata_cell.obs,\n",
    "    x=\"bin_count\",\n",
    "    kde=True,\n",
    "    bins=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc0f50-3699-4674-9e46-31b23970820a",
   "metadata": {},
   "source": [
    "Now, we can QC, cluster and analyse the data in the same way as before. Here again we apply a very quick filter to remove cells with low transcript counts and with very few or very large number of bins assigned to a cell - in practice, you should explore various QC thresholds to find a more optimal strategy. \n",
    "\n",
    "!!! Our filter is probably overly stringent, in that we remove a lot of cells from the analysis. This will help with the speed of this tutorial, but in practice you probably want to keep more cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df18d2-63f1-4fd8-8c39-be7e75068542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = (adata_cell.obs[\"bin_count\"] > 3) & (adata_cell.obs[\"total_counts\"] > 200) & (adata_cell.obs[\"bin_count\"] < 25)\n",
    "\n",
    "print(f\"Barcodes before filtering: {adata_cell.n_obs}\")\n",
    "\n",
    "adata_cell = adata_cell[mask].copy()\n",
    "\n",
    "print(f\"Barcodes after cell count filter: {adata_cell.n_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b173b-4ba8-4570-a1eb-54d4d6d9d3ee",
   "metadata": {},
   "source": [
    "Next, we normalise and cluster the data as usual. As we have quite a large number of cells, this step can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b77ce-d850-4923-a0b3-c7f8bda735a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(\n",
    "    adata_cell,\n",
    "    inplace=True\n",
    ")\n",
    "sc.pp.log1p(\n",
    "    adata_cell\n",
    ")\n",
    "sc.pp.highly_variable_genes(\n",
    "    adata_cell,\n",
    "    flavor=\"seurat\",\n",
    "    n_top_genes=5000\n",
    ")\n",
    "sc.pp.pca(\n",
    "    adata_cell\n",
    ")\n",
    "sc.pp.neighbors(\n",
    "    adata_cell,\n",
    "    n_pcs=20\n",
    ")\n",
    "sc.tl.umap(\n",
    "    adata_cell\n",
    ")\n",
    "sc.tl.leiden(\n",
    "    adata_cell,\n",
    "    key_added=\"clusters\",\n",
    "    flavor=\"igraph\",\n",
    "    n_iterations=2,\n",
    "    resolution=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a45eec-b0b1-4534-b6bf-76ec7810dd48",
   "metadata": {},
   "source": [
    "Now, we can visualise the data as before. The dataset is sparser than the 8um bins and depending on your biological question, bin level aggregation may be more suitable. \n",
    "\n",
    "How does the data look like if we use only nuclei-segmentation or just expression density segmentation results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17433305-5fa1-4508-8509-dbf47abd34e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    adata_cell,\n",
    "    color=[\"clusters\"],\n",
    "    wspace=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497b832-c81b-489f-966c-5b8a48753473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq.pl.spatial_scatter(adata_cell, shape=None, color=\"clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5949cc46-3dd1-4e6b-962e-878927b22ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_cell, color=[\"Myh11\", \"Cd74\", \"Epcam\"], vmax=\"p99\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e106951-73df-4d3d-8e62-58e9a41c0ef8",
   "metadata": {},
   "source": [
    "Save cell segmented [AnnData](https://anndata.readthedocs.io/en/stable/) object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee092121-887e-476f-b663-849c9a99780d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.write(os.path.join(OUTPUT_FOLDERNAME, f'day2_hd_cell_segmented.h5ad'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
